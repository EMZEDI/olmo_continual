{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt, Response 1, and Response 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preference data generation prompting\n",
    "\n",
    "with open(\"../data/constitution_1.txt\", \"r\") as f:\n",
    "    constitution = f.read()\n",
    "\n",
    "# num_data_points = 10 # could go up to 8000 for a single query\n",
    "num_data_points = 10\n",
    "domains = [\n",
    "    \"Health and Well-being\",\n",
    "    \"Education and Learning\",\n",
    "    \"Workplace and Employment\",\n",
    "    \"Social Media and Online Behavior\",\n",
    "    \"Environmental Sustainability\",\n",
    "    \"Human Rights and Social Justice\",\n",
    "    \"Technology and Privacy\",\n",
    "    \"Family and Relationships\",\n",
    "    \"Legal and Ethical Dilemmas\",\n",
    "    \"Politics and Governance\"\n",
    "]\n",
    "\n",
    "\n",
    "GENERATOR_PROMPT = \\\n",
    "    f\"\"\"{constitution}\n",
    "\n",
    "    Given the constitution listed above, create a dataset with three columns: a prompt, Response 1, and Response 2. The prompt should describe a common scenario or situation or state in the world. Response 1 should obey the list of constitutions provided, ensuring it aligns with ethical principles like equality, respect, and fairness. Response 2 should be a counterexample that doesn't follow the constitutions and violates one or more of these principles. You dont need to start your prompt by saying \"User asks\". Ensure the prompts cover a variety of everyday situations only in f{domains[0]} context. Give me f{num_data_points} such examples.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class PreferenceDataPoint(BaseModel):\n",
    "    prompt: str\n",
    "    response_1: str\n",
    "    response_2: str\n",
    "\n",
    "    def get_words(self):\n",
    "        return len(self.prompt.split()) + len(self.response_1.split()) + len(self.response_2.split())\n",
    "\n",
    "class PreferenceDataset(BaseModel):\n",
    "    row: list[PreferenceDataPoint]\n",
    "\n",
    "    def to_csv(self, path: str):\n",
    "        with open(path, \"w\") as f:\n",
    "            f.write(\"prompt,response_1,response_2\\n\")\n",
    "            for row in self.row:\n",
    "                response_1 = row.response_1.replace(\"Response 1:\", \"\").strip()\n",
    "                response_2 = row.response_2.replace(\"Response 2:\", \"\").strip()\n",
    "                f.write(f'\"{row.prompt}\",\"{response_1}\",\"{response_2}\"\\n')\n",
    "\n",
    "    def get_words(self):\n",
    "        return sum([row.get_words() for row in self.row])\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"\"},\n",
    "        {\"role\": \"user\", \"content\": GENERATOR_PROMPT},\n",
    "    ],\n",
    "    response_format=PreferenceDataset,\n",
    ")\n",
    "\n",
    "event = completion.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens used for 10 data points: 832.0\n",
      "Total tokens used for input: 1346.6666666666667\n",
      "Total cost for input w. 10 outputs and 1/10 context: $0.0007012\n",
      "Total cost for input w. 1000 outputs and 1/10 context: $0.050122\n",
      "Total cost for input w. 10000 outputs and 1/10 context: $0.49940199999999996\n"
     ]
    }
   ],
   "source": [
    "event.to_csv(\"../data/preference_data_1.csv\")\n",
    "output_tokens_for_10_datapoints = (event.get_words() * 100) / 75\n",
    "print(f\"Total tokens used for 10 data points: {output_tokens_for_10_datapoints}\")\n",
    "input_tokens = (len(GENERATOR_PROMPT.split()) * 100) / 75\n",
    "print(f\"Total tokens used for input: {input_tokens}\")\n",
    "output_cost_for_10_gen = (output_tokens_for_10_datapoints * 0.6) / 1000000\n",
    "input_cost = (input_tokens * 0.15) / 1000000\n",
    "total_cost = output_cost_for_10_gen + input_cost\n",
    "print(f\"Total cost for input w. 10 outputs and 1/10 context: ${total_cost}\")\n",
    "print(f\"Total cost for input w. 1000 outputs and 1/10 context: ${input_cost + (output_cost_for_10_gen * 100)}\")\n",
    "print(f\"Total cost for input w. 10000 outputs and 1/10 context: ${input_cost + (output_cost_for_10_gen * 1000)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of runs per context and half query: 50.0\n",
      "Total cost for one context: $0.48999\n"
     ]
    }
   ],
   "source": [
    "# given that there is $0.5 for around 8000 outputs for one context and one query, we do this 2 times with higher temperature per each context giving 20 * 8000 outputs with $10 for the cost.\n",
    "\n",
    "\n",
    "# since max output tokens for the model is 16000, and we want to generate 800000 output tokens:\n",
    "num_runs_per_context_and_half_query = 800000 / 16000\n",
    "print(f\"Number of runs per context and half query: {num_runs_per_context_and_half_query}\")\n",
    "output_cost_for_160_gen = (16000 * 0.6) / 1000000\n",
    "total_cost_for_one_context = num_runs_per_context_and_half_query * (input_cost + output_cost_for_160_gen)\n",
    "print(f\"Total cost for one context: ${total_cost_for_one_context}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preference rating by Llama 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file error checking\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "const_number = 1\n",
    "direct = f\"../data/preferences/{const_number}\"\n",
    "\n",
    "all_files = os.listdir(direct)\n",
    "dataframes = []\n",
    "for file in all_files:\n",
    "    file_path = os.path.join(direct, file)\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        dataframes.append(df)\n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f\"Parsing error in file {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 35842 entries, 0 to 16\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   prompt      35842 non-null  object\n",
      " 1   response_1  35842 non-null  object\n",
      " 2   response_2  35842 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# compile all csv files into one\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "NUM_runs=200\n",
    "NUM_classes=10\n",
    "const_number=1\n",
    "\n",
    "direct = f\"../data/preferences/{const_number}\"\n",
    "# for all files in the dir, compile into one csv with columns, prompt, response_1, response_2\n",
    "all_files = os.listdir(direct)\n",
    "df = pd.concat([pd.read_csv(f\"{direct}/{file}\") for file in all_files])\n",
    "df.info()\n",
    "df = df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 1\n",
      "Python-dotenv could not parse statement starting at line 2\n",
      "Python-dotenv could not parse statement starting at line 5\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.33s/it]\n",
      "/home/mila/s/shahrad.mohammadzadeh/projects/olmo_continual/.venv/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  7.06it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  7.36it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 104\u001b[0m\n\u001b[1;32m    101\u001b[0m     p\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m processes:\n\u001b[0;32m--> 104\u001b[0m     \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Combine the results\u001b[39;00m\n\u001b[1;32m    107\u001b[0m df_results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(return_dict\u001b[38;5;241m.\u001b[39mvalues(), ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_pid \u001b[38;5;241m==\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a child process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a started process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_popen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[38;5;241m.\u001b[39mdiscard(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/popen_fork.py:43\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWNOHANG\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/popen_fork.py:27\u001b[0m, in \u001b[0;36mPopen.poll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m         pid, sts \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;66;03m# Child process not yet created. See #1731717\u001b[39;00m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;66;03m# e.errno == errno.ECHILD == 10\u001b[39;00m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# evaluate function\n",
    "def evaluate_preference_batch(rows, constitution, pipe):\n",
    "    prompts = []\n",
    "    for row in rows:\n",
    "        prompt = row['prompt']\n",
    "        response_1 = row['response_1']\n",
    "        response_2 = row['response_2']\n",
    "\n",
    "        # Construct the input text\n",
    "        input_text = f\"\"\"{constitution}\n",
    "        Given the constitution above and the following prompt:\n",
    "\n",
    "        \"{prompt}\"\n",
    "\n",
    "        Here are two responses:\n",
    "\n",
    "        Response 1: \"{response_1}\"\n",
    "\n",
    "        Response 2: \"{response_2}\"\n",
    "\n",
    "        Based on the constitution, which response better adheres to the guidelines? Respond with \"0\" if Response 1 is better or \"1\" if Response 2 is better.\"\"\"\n",
    "        prompts.append(input_text)\n",
    "\n",
    "    # Generate the model's responses in batch\n",
    "    outputs = pipe(prompts, max_new_tokens=10, return_full_text=False)\n",
    "\n",
    "    preferences = []\n",
    "    for output in outputs:\n",
    "        generated_text = output['generated_text']\n",
    "        preference = ''.join(filter(str.isdigit, generated_text.strip()))\n",
    "        if preference in ['0', '1']:\n",
    "            preferences.append(int(preference))\n",
    "        else:\n",
    "            preferences.append(None)\n",
    "    \n",
    "    return preferences\n",
    "\n",
    "# run inference\n",
    "def worker(device_id, df_chunk, constitution, return_dict):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(device_id)\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "    # Load the model and tokenizer within the process\n",
    "    model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=\"auto\",\n",
    "    )\n",
    "\n",
    "    # Set up the pipeline\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    batch_size = 8  # Adjust the batch size as needed\n",
    "    preferences = []\n",
    "    for i in range(0, len(df_chunk), batch_size):\n",
    "        batch = df_chunk.iloc[i:i+batch_size]\n",
    "        batch_preferences = evaluate_preference_batch(batch.to_dict('records'), constitution, pipe)\n",
    "        preferences.extend(batch_preferences)\n",
    "\n",
    "    df_chunk = df_chunk.copy()\n",
    "    df_chunk['preference'] = preferences\n",
    "    return_dict[device_id] = df_chunk\n",
    "\n",
    "# Split the DataFrame into chunks for each GPU\n",
    "num_gpus = 2\n",
    "df_chunks = np.array_split(df, num_gpus)\n",
    "\n",
    "with open(\"../data/constitution_1.txt\", \"r\") as f:\n",
    "    constitution = f.read()\n",
    "\n",
    "manager = mp.Manager()\n",
    "return_dict = manager.dict()\n",
    "processes = []\n",
    "for i in range(num_gpus):\n",
    "    p = mp.Process(target=worker, args=(i, df_chunks[i], constitution, return_dict))\n",
    "    processes.append(p)\n",
    "    p.start()\n",
    "\n",
    "for p in processes:\n",
    "    p.join()\n",
    "\n",
    "# Combine the results\n",
    "df_results = pd.concat(return_dict.values(), ignore_index=True)\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
